---
title: "Lab 2"
author: "Karlena Ochoa, Tamara Niella, Kathryn Denning"
date: "4/14/2020"
output: 
  html_document:
    toc: true
    toc_float: true
    dev: png
  pdf_document:
    dev: cairo_pdf
    latex_engine: xelatex
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(rio)
library(here)
library(tidyverse)
library(tidymodels)
library(vip)
library(Cairo)
library(vroom)

col_specs <- cols(
  .default = col_character(),
  id = col_double(),
  attnd_dist_inst_id = col_double(),
  attnd_schl_inst_id = col_double(),
  enrl_grd = col_double(),
  calc_admn_cd = col_logical(),
  partic_dist_inst_id = col_double(),
  partic_schl_inst_id = col_double(),
  lang_cd = col_character(), #<<
  score = col_double(),
  classification = col_double(),
  ncessch = col_double(),
  lat = col_double(),
  lon = col_double()
)

d <- vroom::vroom(here::here("data", "train.csv"),
                  col_types = col_specs) %>% 
  select(-classification)  %>% 
  mutate(lang_cd = ifelse(is.na(lang_cd), "E", lang_cd),
         ayp_lep = ifelse(is.na(ayp_lep), "G", ayp_lep),
         tst_dt = lubridate::as_date(lubridate::mdy_hms(tst_dt))) %>% 
  sample_frac(.25) %>% 
  janitor::remove_empty(c("rows", "cols")) %>% 
  drop_na() %>% 
  select_if(~length(unique(.x)) > 1)
```

# 1. Split the data

Set a seed and split the data. Do not stratify by any variables. Extract the training data from this split.

```{r split data}
set.seed(123)
splt <- initial_split(d)
train <- training(splt)
train
```

# 2. Evaluate variable importance

Use {vip} to identify the most important variables related to score in the training data. Remember to remove ID variables from the dataset prior to evaluating variable importance (e.g., select(train, -contains("id"), -ncessch))).

```{r vip var importance}
train2 <- select(train, -contains("id"), -ncessch)

vip_1 <- vip(lm(score ~ ., train2))
```

Produce a plot displaying a rank ordering of variables by their importance.

```{r vip plot 1}
vip_1
```

Produce at least three additional plots displaying the relation between a given variables of your choosing and the outcome.

```{r vip plot 2}
vip_2 <- vip(lm(score ~ tag_ed_fg*sp_ed_fg, train2))
vip_2
```

The above plot shows the interaction between tag and sped, which isn't adding much to the model.

```{r vip plot 3}
vip_3 <- vip(lm(score ~ ethnic_cd, train2))
vip_3
```

The above plot shows that examining ethnity (Hispanic and black) may be good to include in model.

```{r vip plot 4}
vip_4 <- vip(lm(score ~ econ_dsvntg*ethnic_cd, train2))
vip_4

vi_4 <- vi(lm(score ~ econ_dsvntg*ethnic_cd, train2))
vi_4
```

It appears the interaction isn't adding much. It appears that the main effect of the variables is more important in the model predcition than the interactions of the variables.

# 3. Fit a preliminary model

Fit a model using as many variables as you’d like, using main effects only (i.e., no interactions) and only linear terms (i.e., no polynomial fits). Estimate the RMSE and R2 for the given model using k-fold cross-validation.

```{r prelim model}
cv <- vfold_cv(train)

mod <- linear_reg()
mod

mod <- mod %>% 
  set_engine("lm")
mod

mod <- mod %>% 
  set_mode("regression")
mod

baseline_rec <- recipe(score ~ econ_dsvntg + ethnic_cd + sp_ed_fg, train)

baseline_rec

m1 <- mod %>% 
  fit_resamples(preprocessor = baseline_rec,
                resamples = cv)
m1

#rmse and r2 are default 

m1_metrics <- m1 %>% 
  select(id, .metrics) %>% 
  unnest(.metrics) %>% 
  pivot_wider(names_from = .metric, 
              values_from = .estimate)

m1_metrics
```

It’s probably easiest if you use a recipe. For this portion you only need to add the formula and the data source. For example, something like the below


# 4. Refine the model

Fit a new model that either (a) provides a better estimate of out-of-sample predictive accuracy, or (b) provides a similar estimate of out-of-sample predictive accuracy with a smaller number of variables. Feel free to use interactions, polynomials, etc. Note if you’re trying to fit interactions/polynomials you will likely need to do so within a recipe, additing additional steps for step_interact() and/or step_poly. Additionally, you’ll need to explicitly dummy code your nominal variables before creating interactions with step_dummy(all_nominal()). This will actually change the names of your columns, so when creating interactions in the steps that follow, it’s easiest to use starts_with helper functions, e.g., step_interact(terms = ~ starts_with("econ_dsvntg") + starts_with("sp_ed_fg")). Alternatively, you can stick with a main-effects only model for now.

assessment sample

Aiming for more predictive model by adding tag. 
```{r model refine}


rec_1 <- recipe(score ~ econ_dsvntg + ethnic_cd + sp_ed_fg + tag_ed_fg, train)

m2 <- mod %>% 
  fit_resamples(preprocessor = rec_1,
                resamples = cv)
m2


rec_1_metrics <- m2 %>% 
  select(id, .metrics) %>% 
  unnest(.metrics) %>% 
  pivot_wider(names_from = .metric, 
              values_from = .estimate)

rec_1_metrics


```

The metrics from the refined model had a slightly smaller RMSE, and better estimates for out of sample predictive accuracy.  

# 5. Make predictions on your test dataset

Complete each bullett below. Text and code between bulletts is intended to be clarifying.

- Fit your final model, developed in Question 4, to your full training dataset.
Note, you can do this by first extracting your full, prepped dataset from your recipe with something like

prepped_train <- rec %>% 
  prep() %>% 
  juice()
where rec is your recipe. If you then look at prepped_train, you’ll see categorical variables have been dummy coded, and any interactions you’ve made, etc., will be new columns in the data frame.

Second you can fit the full model with something like

full_train_fit <- fit(mod, score ~ ., prepped_train)
where mod is your model object (e.g., linear_reg()), score ~ . is shorthand syntax to imply that the score variable is modeled by all predictors, and prepped_train is your training data that have been processed by your recipe, as above.

```{r fit final model}


```

- Extract your test dataset (if you haven’t already)

```{r extract test dataset}


```

- Apply your recipe to your test dataset

This implies making the same preprocessing steps to the test dataset that we did to our training dataset. The code is the same, except instead of using juice we use bake and we have to tell it the new dataset to apply to steps to. For example, something like the below

prepped_test <- rec %>% 
  prep() %>% 
  bake(test)

```{r apply recipe to test}

```

- Make predictions against your test dataset and calculate RMSE and R2.

There are many ways to do this, but I would add a new column to your prepared testing dataframe that includes the predictions from your model. Note, you are not fitting your model to the test dataset. Rather, you create a new column in your test dataset with predictions from your previously fit model, using the predict function.

When calculating RMSE and R2, you may want to use the rmse and rsq functions from the {{yardstick}} package, which is loaded with tidymodels.

```{ r test preds}


```

Bonus
For up to one point extra credit, read in test.csv and make predictions from your fitted model on this new dataset. Note that you will again need to apply your recipe to the dataset before you can make predictions. You’ll also need to create a temporary variable for the outcome (e.g., mutate(score = 1)) to get the recipe to work. In other words, your code for reading in the data should look something like:

kaggle_test <- vroom::vroom(
  here::here("data", "edld-654-spring-2020", "test.csv"),
  col_types = col_specs
  ) %>% 
  mutate(lang_cd = ifelse(is.na(lang_cd), "E", lang_cd),
         ayp_lep = ifelse(is.na(ayp_lep), "G", ayp_lep),
         tst_dt = lubridate::as_date(lubridate::mdy_hms(tst_dt))) %>% 
  mutate(score = 1)
You should then be able to apply the same recipe on it that you applied above, again using the rec %>% prep() %>% bake() workflow, make predictions, and write out the CSV. Make sure the CSV matches the sample-submission.csv file, with an Id (equivalent to test$id) and Predicted columns.

One complication here is that our predictions cannot be NA and we must make predictions for every row in the test dataset. We will discuss methods for handling missing data later in the class. For now, just recode NA’s in your prediction output to match the overall sample mean.

```{r bonus}

```
